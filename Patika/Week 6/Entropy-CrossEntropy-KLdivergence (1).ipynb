{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67ee66fd",
   "metadata": {},
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdad1d0",
   "metadata": {},
   "source": [
    "#### In information theory, the entropy of a random variable is the average level of \"information\", \"surprise\", or \"uncertainty\" inherent in the variable's possible outcomes. The concept of information entropy was introduced by Claude Shannon in his 1948 paper \"A Mathematical Theory of Communication\" and is sometimes called Shannon entropy in his honour.\n",
    "\n",
    "\n",
    "#### in a more understandable way, entropy is the amount of information an event or message contains. In other words, it can be expressed as how surprising an event is. We store information in bits on our computers. While performing the storage process, we try to store the information with the minimum number of bits, because an increase in the number of bits means an increase in the energy to be spent.\n",
    "\n",
    "#### To find the amount of information an event contains, we can use the probability of that event. When we formulate this, we get the following formula.\n",
    "\n",
    "#### h(x) = -log2( p(x) ) where p(x) is probability of event x occurring.\n",
    "\n",
    "#### The reason we do the operation in logarithm base 2 is that we display the information in binary digits on our computers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528e829f",
   "metadata": {},
   "source": [
    "#### Let's try to understand these concepts better with examples. Let's have a fair coin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e002b5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0   1.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# p(tail)=0.5\n",
    "# p(head)=0.5\n",
    "#lets calculate amount of informations of these events\n",
    "h_p_tail=-math.log2(0.5)\n",
    "h_p_head=-math.log2(0.5)\n",
    "print(h_p_tail, \" \", h_p_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0437b3f4",
   "metadata": {},
   "source": [
    "#### Since the probability of occurrence of these events is equal, the amount of information they carry about the event is equal and equal to 2.\n",
    "\n",
    "#### let's assume our coin is unfair and the probability of getting heads and tails are not equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1803443a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0   0.4150374992788438\n"
     ]
    }
   ],
   "source": [
    "# p(tail)=0.25\n",
    "# p(head)=0.75\n",
    "#lets calculate amount of informations of these events\n",
    "h_p_tail=-math.log2(0.25)\n",
    "h_p_head=-math.log2(0.75)\n",
    "print(h_p_tail, \" \", h_p_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2fb148",
   "metadata": {},
   "source": [
    "#### Therefore, as we mentioned before, events with low probability contain a larger amount of information about that event.\n",
    "\n",
    "#### Now, if we want to calculate the information for a random variable X, we need to look at the expected information over all the actual events. Here we can use the Shannon Entropy formula, which is one of the entropy formulas and is used very often."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ee4d35",
   "metadata": {},
   "source": [
    "![Shannon Entropy Formula](https://www.walletfox.com/course/shannonSource/shannon_formula.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb8b9a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Expected Information of Fair Coin\n",
    "exp_fair_info=- (0.5*math.log2(0.5) + 0.5*math.log2(0.5))\n",
    "exp_fair_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "086e8e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8112781244591328"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Expected Information of Unfair Coin\n",
    "exp_unfair_info=- (0.25*math.log2(0.25) + 0.75*math.log2(0.75))\n",
    "exp_unfair_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea42e8f",
   "metadata": {},
   "source": [
    "#### When we look at the unfair coin, the result is more likely to land heads, as the probability of getting heads is higher. In other words, the surprise in the event is lower than the equal probability coin flip. So entropy will be lower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f275e4a",
   "metadata": {},
   "source": [
    "## Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e844ff5",
   "metadata": {},
   "source": [
    "#### Cross-Entropy is the expected entropy value for the Q probability distribution we find while the true probability distribution is P. It measures the relative entropy between two probability distributions over the same set of events. When we look at the Cross-Entropy formula;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f33f5e",
   "metadata": {},
   "source": [
    "![Shannon Entropy Formula](https://miro.medium.com/max/700/1*koxFBp0VFEzqeiNL9diaUw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9068a95f",
   "metadata": {},
   "source": [
    "#### Let P be the true label distribution and Q be the predicted label distribution. Suppose the true label of one particular sample is B and our classifier predicts probabilities for A, B, C as (0.15, 0.60, 0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2af32f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss: 0.7369655941662062\n"
     ]
    }
   ],
   "source": [
    "# True label distribution: P(A)=0.00, P(B)=1.00, P(C)=0.00\n",
    "# Predicted Label Distribution: q(A)=0.15, q(B)=0.60, q(C)=0.25\n",
    "H_p_q = -(0 * math.log2(0.15) + 1 * math.log2(0.6) + 0 * math.log2(0.25))\n",
    "print(\"Cross Entropy Loss:\" , H_p_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e106fa",
   "metadata": {},
   "source": [
    "#### On the other hand, if our classifier was more confident and predicted probabilities as (0.05, 0.90, 0.05), we would get cross-entropy as 0.15, which is lower than the above example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3747436e",
   "metadata": {},
   "source": [
    "### KL Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230cfecc",
   "metadata": {},
   "source": [
    "#### KL Divergence = Cross Entropy - Entropy or it can be formulated like this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13160045",
   "metadata": {},
   "source": [
    "#### It is the amount of information we lose when we use the Q probability distribution to estimate the P probability distribution. We obtain the KL divergence formula as above. KL divergence of 0 indicates that the P and Q distributions are the same. The belove formula can be used for discrete probability distributions for the P and Q distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac06422",
   "metadata": {},
   "source": [
    "![KL Divergence Formula](https://www.statisticshowto.com/wp-content/uploads/2016/10/kl-divergence-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6266a6",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9b3f97",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Entropy_(information_theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af81b61b",
   "metadata": {},
   "source": [
    "https://medium.com/kaveai/d%C3%BCzensizlik-entropy-%C3%A7apraz-d%C3%BCzensizlik-cross-entropy-ve-kl-iraksakl%C4%B1%C4%9F%C4%B1-kl-divergence-89d26735789f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9424d6f",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=ErfnhcEV1O8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35138e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
